\documentclass[12pt,a4paper]{article}
\usepackage{lifis}
\usepackage[utf8]{inputenc}

\title{Probleme der Forschung zum\\ digitalen Wandel – eine Propädeutik\\ der
  Digital Humanities}

\author{Ken Pierre Kleemann, Leipzig}
\def\theauthor{Ken P. Kleemann}
\date{14.\,12.\,2018}
\begin{document}
\maketitle

\begin{quote}
In diesem Aufsatz wird über die Erfahrungen eines Interdisziplinären
Lehrprojekts berichtet, mit dem an der Leipziger Universität seit mehreren
Jahren Fragen des „Digitalen Wandels“ im grundständigen Studium von und
gemeinsam mit Studierenden der Informatik und der Geisteswissenschaften
thematisiert und analysiert sowie mit eigenen praktischen Erfahrungen
abgeglichen werden.  Im Zuge der Umsetzung dieses Lehrprojekts wurde immer
deutlicher, dass wir damit ein Thema behandeln, das andere unter der
Überschrift „Digital Humanities“ besprechen. Unsere Lehrerfahrungen liefern
also zugleich einen fundierten Beitrag zu einer „Propädeutik der Digital
Humanities“. Diese Lehrerfahrungen sind im folgenden Aufsatz genauer
ausgearbeitet.
\end{quote}

\section*{1. Gesellschaftliche Strukturen im digitalen Wandel}

Begriffsarbeit spielt in der Informatik im Bereich semantischer Technologien
eine zentrale Rolle, auch wenn praktische Anwendungen heute vor allem mit der
Übertragung bereits institutionalisierter Begriffssysteme ins Digitale und
damit der Aufbereitung entsprechend etablierter Praxen für maschinelle
Unterstützung befasst sind. Allerdings treten hierbei immer wieder
Widersprüchlichkeiten als Kooperationshemmnisse zu Tage und geben Anlass zu
Verdruss und Reflektion.

Derartige Fragestellungen stehen im Mittelpunkt eines Interdisziplinären
Lehrprojekts\footnote{Siehe dazu
  \url{http://bis.informatik.uni-leipzig.de/de/Lehre/Graebe/Inter}.} an der
Leipziger Universität, in dem bereits im 11. Durchgang versucht wird,
begriffliche Aspekte des Digitalen Wandels sprechbar zu machen. Mit Vorlesung,
Seminar und Projektpraktika wird einerseits Begriffsarbeit (Vorlesung)
geleistet, andererseits werden studentische Erfahrungen reflexiv aufgegriffen
(Seminar) und diese in ausgewählten Praxisprojekten vertieft.

„Gesellschaftliche Strukturen im digitalen Wandel“ ist nicht nur der
Arbeitstitel dieses Lehr- und Forschungsangebots, sondern auch Anzeige eines
umfassenden Problems. Der Begriff \emph{Digitaler Wandel} -- wenn nicht gar
\emph{Revolution} -- gehört mittlerweile zur Grundausstattung jedes
feuilletonistisch aktiven Schreiberlings und noch mehr zur
öffentlichkeitswirksamen Diskussion über die Transformationen, welche seit
rund dreißig Jahren die etablierten Kategorien der Beschreibung unserer
geteilten Welt zerreißen.

Dass etwas Umfassendes geschieht, dass die Lebensgewohnheiten und
Interaktionsmuster des Menschen sich ändern, ist unbestritten, und jeder
Einzelne ist sowohl Betroffener als auch Beobachter einer Änderung, welche nur
in ihrem Intensitätsgrad in Frage steht. Der Begriff Digitaler Wandel oder
Revolution zeigt nicht nur die persönlichen Erfahrungen über veränderte
Gewohnheiten und Abfolgen des alltäglichen Lebens an, sondern auch veränderte
politische und gesellschaftliche Konstellationen, welche ehemals gefestigte
Verfahrensweisen aus dem Tritt bringen oder sogar gänzlich aus dem Alltag
verbannen. Gesellschaftliche Strukturen im digitalen Wandel soll somit nicht
eine strukturalistische Lesart der Gesellschaftstheorie beschwören, sondern
durch die persönliche wie auch gesellschaftliche Erfahrung nicht nur auf die
Änderungen eines dem Menschen vermeintlich äußerlichen Ordnungszusammenhangs
verweisen, sondern auf die Änderung der Strukturen, welche uns auch zu dem
machen, was wir jetzt glauben zu sein. Ein Außen und Innen, eine Technik, die
dem Menschen gegenübersteht, ihn beherrscht oder gar entfremdet, ist mehr und
mehr ein Narrativ, das man kleinen Kindern erzählt, um dystopische Ängste zu
wecken und um sie unter ihre Decke zu verbannen.

Ersichtlich wird, dass das Hauptproblem tatsächlich die Verständigung über
Bedeutungen unserer Beschreibungen und ihrer realweltlichen Rückkopplungen
ist.  Interdisziplinarität ist ein erster Schritt, doch bei weitem ungenügend,
wenn es zu einer fruchtbaren Synthese von Lehre und Forschung kommen soll.
Infradisziplinarität\footnote{Wir verwenden den Ausdruck, um deutlich zu
  machen, dass es um \emph{gemeinsame Arbeit am Begriff} geht und nicht nur,
  wie im Neologismus „Transdisziplinarität“, um ein gemeinsames
  \emph{Festsetzen} von Begriffen.}, das Entwickeln und Gebrauchen gemeinsamer
Begriffe, wird somit für eine umfassende Lehre und Forschung notwendig, umso
mehr, als wir gerade persönlich und gesellschaftlich in einem
Transformationsprozess stecken, der selbst kaum verstanden
wird. \emph{Gesellschaftliche Strukturen im digitalen Wandel} ist somit eine
Beschreibung, ein Problem, eine Auf"|forderung und eine Anleitung für eine
verschränkte Lehre und Forschung, welche interdisziplinär wie auch
infradisziplinär vermeintliche empirische Phänomene der Entwicklung
untersuchen, hinterfragen, kontextualisieren, analysieren und kritisieren
will.

Seit rund sieben Jahren war es möglich, an der Leipziger Universität ein
derartiges Vorhaben zu institutionalisieren und mit unterschiedlichsten
Akteuren, in unterschiedlichsten Formaten die unterschiedlichsten Diskussionen
zu führen und Schritt für Schritt eine gewisse Klarheit und begriff"|liche
Schärfung zu erreichen. \emph{Gesellschaftliche Strukturen im digitalen
  Wandel} soll als Arbeits- und Forschungstitel die Aufmerksamkeit auf alle
Facetten der Änderungen legen, welche durch das Web 2.0 getragen werden.
\enlargethispage{1em}

\section*{2. „Web 2.0“ und Semantische Technologien}

\emph{Web 2.0} ist ein Beispiel und ein echtes Problem für infradisziplinäres
Arbeiten. Liest man das Feuilleton der letzten Jahre, könnte man meinen,
gerade dieser wichtige Begriff sei mehr als klar und selbstverständlich. In
dieser normalen Sicht wird damit mittlerweile die vermeintlich neue Qualität
des Internets beschrieben, welche von riesigen Unternehmen strukturiert und
durch Plattformen, als soziale Netzwerke, zusammengehalten wird. Web 2.0 meint
hier lediglich die quantitative Zunahme von Daten, welche als neue Qualität
vom sogenannten Plattformkapitalismus vorangetrieben wird; Web 2.0 ist gleich
Internet plus soziale Netzwerke.

Ursprünglich, und das nicht einmal vor zehn Jahren, beschrieb dieser Begriff
aber tatsäch"|lich eine neue Qualität. Semantic Web war das eigentliche
Synonym. Hier wurde nicht auf soziale Netzwerke oder den Aufstieg
verschiedenster Plattformen rekurriert, sondern auf technisch-qualitative
Änderungen am Anfang und in der Mitte der 2000er Jahre. Semantic Web ist eine
andere Form des Internets, welche durch sehr spezifische Protokolldependenzen
und von einer sehr speziellen Versprachlichung getragen wird und somit erst
Plattformen, soziale Netzwerke und heutige künstliche Intelligenz möglich
macht.

Zu Beginn des sogenannten Computerzeitalters sehen wir noch die Verwendung
binärer Codes; eine Null bedeutete Strom aus und eine Eins Strom an. Die
Über"|tragung pro Zeiteinheit war hier also sehr wohl eine quantitative
Übertragung elementarer Anweisungen an den Prozessor. Doch schon in den
fünfziger und sechziger Jahren des 20. Jahrhunderts wurden nicht nur
Lochkarten und anschließend Magnetbänder als Speichermedien eingeführt,
sondern auch die Übertragung pro Zeiteinheit in Bits und Bytes normiert. Die
Quantität nahm zu und auch die Qualität der Übertragung pro Zeiteinheit. Doch
nicht genug, in den sechziger und siebziger Jahren wurde auf dieser Basis ein
einheitlicher Zeichensatz und Sprachen vereinbart, welche auf diesem Level
eine Programmierung der Maschinen durch den Menschen einfach und zugänglich
machten. Der Traum der Kybernetik schien sich zu erfüllen, und -- wie Wiener
es schön ausdrückte -- der sich selbst erzeugende Golem war in vermeintliche
Reichweite gerückt\footnote{Norbert Wiener (1965). Gott und Golem Inc.
  Düsseldorf, Wien.}. Diese Maschinen wurden leistungsfähiger und konnten
immer mehr Bytes in Zeichensatzform und damit als Text und Programm speichern
und verarbeiten. Eine künstliche Intelligenz war nun angeblich nur noch eine
Frage eines umfassendes Wörterbuches, das mit einer komplexen Taxonomie dem
Computer eingeimpft werden musste. ELIZA schaffte es, der Sekretärin Joseph
Weizenbaums vorzugaukeln, dass der Computer sie verstehe und auf sie eingehen
könne; ein erfolgreicher Turing-Test? Turing wollte seinen Test immer als
verhaltensmorph verstanden wissen, und ELIZA hatte dieses vermeintliche
Verhalten. Doch wies Weizenbaum, der Schöpfer dieser ersten künstlichen
Intelligenz, darauf hin, dass seiner Sekretärin sehr wohl bewusst war, dass
diese taxonomischen Routinen ein echtes Eingehen auf ihre Sorgen und Nöte nur
simulierte.
\newpage
Weizenbaum selbst sprach, durch dieses gesetzte und programmierte
Wörterbuchproblem angeregt, ELIZA die Fähigkeit eines freien Urteilsgebrauches
ab, und in gewisser Weise fanden die Träume dieser Kybernetik damit bereits in
den achtziger Jahren ihr Ende\footnote{Joseph Weizenbaum (1977). Die Macht der
  Computer und die Ohnmacht der Vernunft. Suhrkamp, Frankfurt/M. }.

Die Entwicklung war damit nicht abgeschlossen. Durch die Einführung des
Desktop Computers wurde die bisherige Verschränkung von Bytes, Zeichensatz und
Programmiersprache komplexer. Die Entwicklung neuer digitaler Sprachartefakte
wie Disk Operating Systems und damit die praktische Verfügbarkeit von Turings
Universalmaschinen führten zu einer noch komplexeren Protokolldependenz in der
Übertragung pro Zeiteinheit. Diese Devices fungierten nun selbst als Rechner
und lösten die Konzentration auf große Datenverarbeitungsanlagen durch
Dezentralität ab.

Mit dem Internet und der einhergehenden Adressierbarkeit dieser Devices durch
das HTTP-Protokoll als oberster Schicht des OSI-Protokollstacks wird die
Komplexität noch gesteigert und eine globale Paketversendung über ein
skalenfreies Netz organisiert, in dem jene digitalen Sprachartefakte
ausgetauscht werden können. Im Internet kursieren keine einfachen Bitströme
von Endgerät zu Endgerät mehr, sondern es erfolgt die Versendung und
Neuzusammensetzung von Paketen auf der Basis dependenter Protokolle. Mit
diesem Internet der neunziger Jahre und des frühen 21. Jahrhunderts wurde es
möglich, statistisch-quantitative Verfahren zu entwickeln, welche nicht nur
über die eigene Website Auskunft gaben, sondern auch über die quantitativen
Aktivitäten aller Beteiligten und damit über die Nutzer selbst. Dieses
Wechselverhältnis war aber bis zu diesem Zeitpunkt nur eine komplexere Version
und quantitative Steigerung der Möglichkeiten der achtziger Jahre, insoweit
nur eine „Beschleunigung“.

Der entscheidende Schritt ereignete sich nun vor knapp 15 Jahren, wo sich eine
weitere qualitative Änderung vollzog, die Vernetzung nicht nur der Rechner,
sondern der Sprachartefakte selbst.  Lokale Akteure, die für ihre
Anwendungsfälle nach kooperativen Lösungen und kooperativem Management
suchten, bedienten sich einer zusätzlichen Versprachlichung auf einer weiteren
Protokollschicht. RDF als Framework stellt eine Sprache bereit, um in
Drei-Wort-Sätzen zu agieren und so etwas wie einen Bibliothekskatalog, ein
Geodatenverzeichnis oder die Fische Alabamas abbildbar zu machen.  Im
Gegensatz zu früheren Protokollschichten wird dabei Sprache aber nicht
verwendet, um technische Interoperabilität zu verhandeln, sondern um
Sprachartefakte selbst miteinander zu vernetzen und damit die menschlichen
Sprachhandlungen selbst maschinell zu unterstützen.  Der Fisch, der als
textuelle Repräsentation erfasst ist und als solche „digital agiert“, wird in
derartigen Sätzen beschrieben und mit einer URI-fizierung referenzierbar.  Die
Leistung besteht nicht nur darin, dass das maschinelle Prozessieren dieses
sprachlich komplexen dependenden Protokollsatzes mehrere Protokollschichten
aufruft, sondern in der Flexibilität solcher Referenzen, als Subjekt, Objekt
oder Prädikat auftreten zu können, welche mit einer bestimmten URI, also einem
Ort im Netz, verbunden sind. Die URI kodiert damit nicht nur die Referenz
selbst, sondern auch deren Gültigkeitskontext. Somit konnten Dinge, Konzepte,
Orte oder numerische Aussagen verbunden werden und als Subjekte auftreten.
Metadaten als textuelle Repräsentationen bilden somit eine komplexe
protokolldependete Beschreibung der Beschreibung unseres alltäglich
vollzogenen Sprachhandelns. Viele Akteure, welche auf hohem technischen Niveau
nur kleine Anwendungslösungen durch spezielle Semantic Web Wörterbücher
suchten, die Ontologien der Informatik und nicht der Philosophie, haben
indirekt eine riesige Abbildung menschlicher begriff"|licher Erfassungen und
Beschreibungsformen erstellt, welche heute als \emph{Linked Open Data
  Cloud}\footnote{Siehe \url{https://lod-cloud.net/}.}  öffentlich verfügbar
ist. Auch die vermeintlich großen Plattformen haben an diesem Prozess
teilgenommen und durch die technische Kompatibilität ihrer Ontologien, wie
Googles Knowledge Graph, an dieser riesigen empirisch gestützten Beschreibung
unserer Beschreibungen der Welt mitgearbeitet. Auf nicht geplantem, nicht
direkt programmiertem Wege haben so die Menschen aus kleinen pragmatischen
Lösungen und regionalen Beschreibungen ihrer Verhaltensmuster eine riesige
technisch abgestimmte sprachlich gefasste Beschreibung ihrer Welt erstellt,
das Web 2.0 oder Internet of Things.

Siri und Alexa, die neuesten Stars und Abkömmlinge dieser Entwicklung, sind
überhaupt nicht mehr mit ELIZA vergleichbar. Diese künstlichen Intelligenzen
werden nicht mehr über programmierte Taxonomien gelenkt, sondern durch die
Austauschprozesse des Web 2.0. Ihnen wird nicht ein Wörterbuch eingeimpft,
sondern ihre Routinen bedienen sich des großen Wörterbuchs, welches das Leben
selbst schreibt. Der Traum der Kybernetik realisiert sich auf vollkommen
anderem Niveau, denn es geht nun nicht um Intelligenz oder verhaltenskonformes
Agieren, sondern um empirische Abgleichungen und Veränderungen des Semantic
Web selbst.

Die Diskussion über zu programmierende Moral oder gar Asimovsche
Robotergesetze mutet fast schon lächerlich an, wenn man bedenkt, dass diese KI
an den Vollzügen der Menschen teilnimmt. Siri und Alexa lernen kein Verhalten,
sie nehmen an menschlichen Performanzen teil und kategorisieren dieses durch
einen Fundus, der die Beschreibungen des menschlichen Verhaltens selbst
beschreibt.  Diese Datenbasis hat somit mit Nullen und Einsen, mit Bitströmen
oder ähnlichen Fiktionen nichts mehr zu tun, auch wenn sie auf diesen
technisch aufsetzt. Metadaten oder Big Data ist nicht einfach die quantitative
Zunahme irgendwelcher Mengen von Datensätzen, sondern eine qualitativ neue
Stufe einer neuen qualitativen Dependenz sehr spezieller Protokolle und einer
dem entsprechenden wirklichen physischen Struktur, nämlich des Internets und
seiner Server, Backbones und Endgeräte.

\section*{3. Herausforderungen zur Begriffsarbeit}

Dieses Problem einer echten qualitativ neuen Entwicklung, welche durch die
Intersubjektivität des Menschen bedingt ist, bringt uns nun in nicht nur
begriffliche Verlegenheit, in kritische Stellung zum alltäglichen dystopischen
Gefasel oder zu interdisziplinär notwendigen Fragen, sondern zu ernsthaften
Problemen der Forschung. Ich möchte im Folgenden ein paar dieser Probleme vor
dem eben skizzierten Hintergrund thematisieren.
\enlargethispage{1em}

\paragraph{1. Menschenbild.}
Auffällig ist, dass sowohl in der Berichterstattung als auch von einschlägigen
Akteuren ein spezifisches Menschenbild verwendet wird. Meist reduziert sich
dieses auf ein Subjekt, das die objektive Welt in Symbolen fasst, mit
Begriffen beschreibt und diese lautverbal mediatisiert. Diese zweistellige
Relation, allein mit Sender und Empfänger als Parametern, kann man vom
philosophischen Standpunkt nur als unzureichend bezeichnen.

Seit dem sogenannten \emph{lingusitic turn}\footnote{Dieser ursprünglich zur
  Diskussion zwischen \emph{ordinary language philosophy} und \emph{idealer
    Sprache} von John R. Searle in den 1960er Jahren geprägte Begriff dient
  heute mindestens als Verweis darauf, die Kopplung unserer geteilten
  Sprachwelten im Handlungsvollzug ernst zu nehmen. } ist gerade dieses
Medien- und Kommunikationsmodell -- als wohlgemerkt anthropologisches Bild --
unter Beschuss, und das zu recht.  Entwicklungspsychologische Forschung,
sprachanalytische Auswertungen und kognitionswissenschaftliche sowie
evolutionäranthropologische Abgleichungen betonen immer mehr die Abhängigkeit
intersubjektiver Vollzugsformen für die Lernleistung von Kindern, mithin
wenigstens eine \emph{dreistellige} Relation der Mediatisierung, in welche
neben Sender und Empfänger auch der \emph{Vollzugskontext} als Parameter
eingeht. Kinder bekommen nicht einfach eine leere Tafel im Kopf beschrieben,
sondern nehmen teil an den schon sprachlich gefassten Vollzugsformen und
Hintergrundannahmen der Gesellschaft, auf die sie -- mit Wittgenstein
gesprochen -- hin abgerichtet werden.

Die Bedeutung eines Begriffs liegt im kontextualisierten Gebrauch und macht es
notwendig, den Ort sozialer Stabilisierung nicht allein im Subjekt zu suchen.
Diesem Intersubjektivitätsproblem und der Erfassung des Wissens über die Welt
gebührt zur Zeit einige Aufmerksamkeit, und es wird versucht, mit Begriffen
wie \emph{practical turn}, \emph{performative turn}, \emph{Hegelian turn} oder
\emph{pragmatic turn} diesem Umstand Rechnung zu tragen.

Für uns bedeutet dies, nicht nur dem typischen Mediatisierungsmodell skeptisch
zu begegnen oder interdisziplinär ein anderes Modell zu erstellen, sondern die
Verbindung zur laufenden Forschung immer wieder zu bekräftigen und an den
eigenen Ansätzen zu arbeiten.

\paragraph{2. Information und Daten.}

Gleichzeitig werden zwei Grundkategorien, nämlich Information und Daten, in
ihrer üblichen Form mehr und mehr fraglich. Information wird hier als
ungeordnete, fast schon roh perzipierte Grundeinheit der Wahrnehmung, also
mithin der semiotischen Namenstaufe verstanden und als mediatisierter Inhalt
zu Daten transformiert. Diese \emph{Phlogiston}, wie Capurro es treffend
bezeichnete\footnote{Rafael Capurro (1998). Das Capurrosche Trilemma.
  \url{http://www.capurro.de/janich.htm}, Abruf am 14.12.2018.}, ist dann
nichts anderes als die symbolische Fähigkeit des Menschen, die Sprache der
Natur zu verstehen, und Computerinformationen haben aufgrund ihrer
mathematischen Form den vermeintlich direkten Kontakt zur Sprache des
Universums.

Dass sich hier ein Menschenbild versteckt, welches mehr als problematisch ist,
habe ich schon erläutert. Durch das Intersubjektivitätsproblem und die
Stabilisierung sozialer Ver"|hältnisse durch den Vollzug sprachlich
explizierbarer Hintergrund"|annahmen und Prä"|suppositionen wird nicht nur das
Mediatisierungsmodell des Menschen fraglich, sondern auch dieses einfache
Abbildverhältnis zur Natur. Frege und Russell haben es schön gesagt: auch eine
Zahl ist nur ein Begriff\footnote{Siehe dazu etwa Thomas Landauer (1997). Die
  Kennzeichnungstheorien von Frege und Russell.
  \url{https://www.landauer.at/die-kennzeichnungstheorien-von-frege-und-russell/},
  Abruf am 14.12.2018.}. Da Begriffe unter anderem mit, durch und für unsere
Handlungskontexte existent sind und uns überhaupt erst als Menschen existieren
lassen, kommen hier nicht nur klassische Fragen der Erkenntnistheorie oder der
Wissenschaftstheorie zum Tragen, sondern echte naturwissenschaftliche
Forschungen zu Fragen der Materie, der Kosmologie, der Evolution und der
Adaption der menschlichen Gattung an die Natur, aber als Teil dieser Natur.

\paragraph{3. Naturwissenschaft und Geisteswissenschaft.}

Es ergibt sich aus dem vorher Gesagten nun konsequent, dass die klassische
Trennung von Naturwissenschaft und Geisteswissenschaft nicht nur veraltet,
sondern als inhaltlich falsch zu bezeichnen ist. Eine Forschung wie auch
Lehre, welche sich für die gesellschaftlichen Strukturen im digitalen Wandel –
und dies im umfassenden Sinn – interessiert und interessieren muss, kann sich
nicht in die Vorstellungen des neunzehnten Jahrhunderts zurückziehen und von
unterschiedlichen Gesetzen oder Gegenständen fabulieren. Dass es eine
unterschiedliche Auf"|fassung von wissenschaftlichen Gesetzen oder gar
verschiedenen Gegenständen gibt, ist aus dem vorherigen komplexen
philosophischen Problem der Intersubjektivität und der Frage des Ortes
sozialer Stabilisierung nicht nur fraglich, sondern selbst ein Ausdruck des
von uns kritisierten Menschen- und Weltbildes, welches hier im Hintergrund
agiert. Interdisziplinarität braucht nicht nur die Ergebnisse der anderen
Disziplinen, sondern selbst reflexive kritische Übersetzung und eine
infradisziplinäre kooperative Fortsetzung.  Am Begriff selbst muss
\emph{gemeinsam} gearbeitet werden.

\paragraph{4. Technik.}

Parallel oder gar inkludiert wird somit ein anderes Verständnis von Technik
notwendig. Immer noch behandelt man, zwar konsequent, aber dennoch fraglich,
Technik als Artefakt, das den Menschen gegenübertritt und ihn aus einer
vermeintlichen Natürlichkeit entfremdet hätte. Dieser Zeugzusammenhang konnte
so den Menschen durch seinen Positivismus und in seiner industriellen
Umgestaltung der Welt in eine sinnentleerte entzauberte Welt bringen, in der
jetzt sogar der Mensch selbst „transhumanisiert“ werden soll.

Dass sowohl von fortschrittsgläubiger als auch skeptisch-dystopischer Seite
hier erneut eine sehr problematische Verbindung von Menschen- und Weltbild
instrumentalisiert wird, dürfte aus dem Vorherigen klar sein, doch lässt sich
eine Alternative nicht einfach formulieren. Genau durch die beschriebenen
Abhängigkeiten der angestrebten Forschung von den Forschungen der anderen
Disziplinen wird nicht nur der kooperative Rahmen gefordert, sondern sehr wohl
die Reaktion auf die tiefer liegenden philosophischen Probleme. Solange es
also nicht gelingt, eine \emph{infradisziplinäre Arbeitsweise} in enger
kooperativer Abstimmung zu erhalten, ist auch ein Technikbegriff, welcher die
intersubjektiven Vollzüge ernst nimmt, nicht zu erreichen.

\paragraph{5. Gesetz und Simulation.}

Um nun nicht einem neuen vermeintlichen Szientismus das Wort zu reden, ist es
notwendig, überhaupt erst einmal Klarheit über die moderne Wissenschaft zu
gewinnen. Allzu oft wird noch immer tradiert, dass die so genannten positiven
Wissenschaften mit einem monokausalen Gesetzesbegriff die Welt angeblich
verstanden hätten.  Diese Sinnentleerung führte zu direkter
Wissenschaftsgläubigkeit, welche als \emph{Fortschritt} die Welt gleich in
mehrere Weltkriege gestürzt hat.

Dieses Geschichtsbild ist direkt abhängig vom vorherigen
Menschen-Weltbild-Komplex und als mehr als falsch zu betiteln.  Seit den
Grundlagenkrisen der Mathematik, Geometrie und Physik am Ende des
19.~Jahrhunderts gibt es kaum einen Naturwissenschaftler, der noch von harten
Gesetzen spricht, geschweige denn von Sinnerklärungen. Konstitutiver Zug
heutiger Naturwissenschaften ist gerade Sinnenthaltung in dem Verständnis,
dass wir sehr viel nicht wissen, aber wenigsten ungefähr die Felder des
Nicht-Wissens abstecken können. Heutige wissenschaftliche Gesetze sind
mitnichten monokausal und brauchen Kontexte wie auch Überprüfungen.

Letztere firmieren heute nicht einfach mehr unter dem Begriff
\emph{Experiment}, sondern werden als \emph{Simulationen} zu prozessualen oder
gar als \emph{Monitoring} zu dauerhaften Abgleichungen von Theorie und Praxis
und somit zur Dynamisierung der wissenschaftlichen Gesetze selbst.

Mit dem \emph{Internet of Things} erhält dieser Prozess der Weiterentwicklung
der epistemischen Grundlagen von Wissenschaft noch einmal einen wesentlichen
Impuls.

\paragraph{6. Digitaler Behaviorismus.}

Zum ersten Mal wird es somit heute möglich, eine Simulation echter
Verhaltensmuster wirklich tätiger Menschen zu prozessieren. Der Ausdruck
Behaviorismus erinnert zwar an Pawlow oder Skinner, dreht sich hier aber nicht
um affektöses oder ähnliches Grundverhalten, das dann für ein klassisches
Mediatisierungsmodell verwendet wird.

Das Web 2.0 ist selbst ungeplanter Ausdruck echter unabgestimmter
Verhaltensmuster und deren tradierten Beschreibungsformen. So wie Siri und
Alexa nicht direkt programmiert werden müssen, so müssen diese Muster nicht
erzeugt werden. Heutige Big Data Analyse ist nicht die taxonomische
Heranführung der experimentellen Architektur an einen Untersuchungsgegenstand,
sondern die Analyse \emph{in Gebrauch befindlicher} speziell versprachlichter
Taxonomien sowie Ontologien, welche über unsere tradierten Taxonomien Auskunft
geben und deren echte Verwendung im sozialen Verhalten darstellen. Ich frage
nicht mehr einfach einen vermeintlichen Datenkorpus ab, sondern die Performanz
gibt mir die Fragen vor.

\paragraph{7. Künstliche Intelligenz.}

So wie ein neuer möglicher digitaler Behaviorismus keine prinzipielle
Festsetzung von unserer Seite braucht, so wenig brauchen moderne künstliche
Intelligenzen die direkte Programmierung ihrer Fähigkeiten.  ELIZA hatte noch
diese Eigenschaften, für Siri oder Alexa sieht die Sache anders aus. Das Web
2.0 macht nicht nur die sensorische Abgleichung mit dem User durch die mobilen
Devices möglich, sondern auch die Abgleichung zum Semantic Web wie auch dessen
Veränderung.  Simulation und Monitoring gehen dabei eine enge Symbiose ein.

Heutige künstliche Intelligenzen sind keine Singletons, welche auf den
jeweiligen Devices eingesperrt sind; eher könnte man von schizophrener
Gleichzeitigkeit reden. Die Anwendung auf den einzelnen Devices macht eine
Individualisierung Alexas möglich, dennoch ist diese zeitgleich in die
Abgleichungen der anderen Devices verstrickt und so im Semantic Web verortet,
welches sich wiederum durch diese Interaktionen folgerichtig verändert. Alexa
braucht genauso wenig einen programmierten Fragen- oder moralischen
Lernkatalog wie ein digitaler Behaviorismus eine grundsätzliche experimentelle
Architektur.

\paragraph{8. Industrie 4.0 und die Zukunft der Arbeitswelt.}

So wenig wie Alexa ein Singleton ist, so wenig ist diese ominöse Industrie 4.0
eine Einführung von Industrierobotern mit einer ELIZA-KI. Mit diesem Begriff
wird die Zukunft der Arbeit thematisiert und damit die Veränderung
gesellschaftlicher Strukturen. Er ist allein deswegen für unser Vorhaben von
Interesse.

Wenn schon heutige KI nicht mehr eine programmierte Taxonomie erfordert, so
erfordert die Robotorisierung der Industrie nicht einfach die Ersetzung des
Menschen durch die Maschine. Unzweifelhaft wird es zu massiven Änderungen von
Berufsprofilen kommen und eine gewisse Freisetzung von Arbeitskraft
insbesondere im primären und sekundären Sektor erfolgen.  Normalerweise wird
nun auf das Anwachsen kreativer Arbeit verwiesen, welche aber selten in ihrem
prekären Status thematisiert wird.

Dabei ist nicht nur die Produktion von Industrierobotern oder die Kopplung zu
KI auf semantisch-technologischer Basis entscheidend. Für das Funktionieren
des Semantic Web im industriellen Maßstab wird die Überlappungsfreiheit von
Ontologien, die kontextspezifische Erstellung von Metadaten, die Big Data
Analyse und das Big Data Mining entscheidend.  Diese Industrie 4.0 macht es
notwendig, nicht so sehr auf Code-Ebene zu agieren, sondern auf der neuen
Protokollebene Personalressourcen auf relativ wiederholbare Arbeiten der
Strukturierung relevanter Sprachartefakte zu konzentrieren. Eine immense Menge
an Arbeitskraft und Arbeitszeit ist notwendig, um eine standort-relevante
Industrie 4.0 aufzubauen, was nicht allein über Breitbandausbau zu erreichen
ist.  Durch eine Anstrengung im dualen Ausbildungssystem, durch Umschulungen
und Fortbildungen wird es möglich, diesem immensen Arbeitsaufwand mit einer
Programmierung zweiter Ordnung zu begegnen.

Möglich wird somit nicht nur und nicht so sehr eine „Kompensation“ der
Auswirkungen des sozialen Wandels, sondern eine nachhaltige Einführung dieser
neuen Strukturen als kooperativer Gestaltungsprozess, welcher weder
ordnungsrechtliche Dogmen noch vertragschlussfähige Problematisierungen
braucht, sondern sich aus den Kontexten selbst speist.

Die Problematik der Entstehung eines neuen Sklaventums oder gar des Endes der
Lohnarbeit möchte ich hier ausklammern. Fest steht, auch hier wird es
notwendig, nicht in eingefahrenen Paradigmen zu denken und noch weniger diese
zu suchen.
\enlargethispage{1em}

\paragraph{9. Paradigmendrehung.}

Wir sahen bisher, dass sowohl klassische Paradigmen des Menschenbildes, des
Weltbildes, der Wissenschaft, der Experimente, als auch der Industrie einen
entscheidenden Dreher erfahren. Aufgrund der Struktur des Semantic Web und
seiner performativen Veränderungen erhalten wir nicht mehr nur Antworten auf
spezifische Fragen, sondern Fragen, welche vorher gar nicht klar waren, und
nun aus dem echten Verhalten unseres Gebrauchs dieser Technologie
entstehen. Für uns als Wissenschaftler ist somit der digitale Behaviorismus
nicht allein das markante Merkmal der technologischen Entwicklung, sondern die
Veränderung der empirischen Basis und der empirischen Erfassung
derselben. Weder kann die klassische positive Wissenschaft diese Komplexität
allein fassen, noch kann das eine wie auch immer abgesonderte Humanitas. Für
eine derartige interdisziplinäre und infradisziplinäre Arbeit wird zum einen
die Kooperation mehr als wichtig, zum anderen wird eine Entwicklung eigener
Forschungsmethoden wie auch eines eigenen Paradigmas bedeutend. Die Digital
Humanities sind zum einen die Reaktion auf ein derartiges Kooperationsgebot,
zum anderen die eigenständige Entwicklung genuiner Methoden und einer sich
entwickelnden Selbstwahrnehmung.

\section*{4. Zu einer Propädeutik der Digital Humanities}

Die Digital Humanities sind ein relativ neues Feld und tatsächlich gerade
dabei, sich selbst zu verstehen; die Frage, ob es ein eigenes Paradigma gibt
und wie dies aussieht, ist zur Zeit Gegenstand lebhafter Diskussionen.

Zuerst hatte dieser Zweig nichts anderes im Sinn als das Scannen und
öffentliche Zugänglichmachen historisch bedeutsamer Texte. Doch schon auf der
Metaebene der Katologisierung und Zuschreibung dieser Texte wurden semantische
Technologien für die Autoren, Orte oder Indexe gebraucht. Eine beeindruckende
Allianz unterschiedlichster Akteure, wie Bibliotheken, Universitäten oder
Vereine, machte sich auf den Weg, ihre kleinen Ausschnitte der Welt zugänglich
zu machen. Kurze Zeit später, vor kaum fünfzehn Jahren, begann man diese Texte
selbst annotationsfähig zu machen, also auch Begriffe und damit Konzepte mit
semantischer Technologie zu erfassen. Heute sind Stilometrie- und
Iterationsanalysen fast schon eine Selbstverständlichkeit für einschlägige
Projekte der Digital Humanities.

Gleichzeitig wird damit unter dem Titel \emph{Mikrohistorik} die Möglichkeit
digitaler behavioristischer Analyse eröffnet. Mehr und mehr berichten Kollegen
der sich unabhängig machenden Disziplin von der Änderung der experimentellen
Ebene. Musste früher eine Architektur des Experimentes entworfen werden,
welche spezifische Fragen an den Korpus stellte, so zeichnet sich immer mehr
ab, dass der Korpus selbst Zusammenhänge freilegt und Fragen produziert.

Die Frage des Paradigmas ist somit nicht allein der Änderung der Methoden
geschuldet oder der notwendigen Interdisziplinarität der Auswertungen, sondern
dem Korpus selbst, welcher sich dynamisch und nicht wie ein statischer
Gegenstand verhält. Erneut sieht man, dass hier nicht nur auf einer
technischen Basis gearbeitet wird oder auf einer fächerübergreifenden Ebene,
sondern auf einem Level, welches einfache Tradierungen klassischer
Menschen-Weltbild-Modelle unmöglich macht.  In unserem Interdisziplinären
Lehrprojekt sind uns dabei im Wesentlichen fünf Formen von „Digital
Humanities“ begegnet.

Eine \emph{erste Form} ist das Scannen und Verfügbarmachen von Texten sowie
die Erstellung von Metadaten. Hier lassen sich schon Methoden der
Korrelationanalyse anwenden und Fragen nach Wer, Mit Wem und Wann beantworten.

Eine \emph{zweite Form} ist das Erstellen und Einpflegen annotationsfähiger
Texte. Hier lassen sich Musterbildungen durchführen und Analysen wie die
Stilometrie anwenden. Es werden Fragen beantwortet nach dem Wie, Was und
Welcher Gruppe ein Autor oder Text angehört.

Die \emph{dritte Form} macht die Drehung des experimentellen Fragens nicht nur
möglich, sondern notwendig. Hier lassen sich Netzwerke, Iterationen und
Kontextabhängigkeiten ermitteln oder, besser gesagt, diese werden durch den
Korpus gegeben. Hier ist die Stelle, die eine Mikrohistorik erst möglich macht
und die Ebene eines digitalen Behaviorismus aufschließt. Damit werden Fragen
nach dem Warum möglich, indem alle anderen vorherigen Fragen der anderen Formen
nicht an den Korpus gerichtet werden, sondern an die im Korpus sichtbar
werdende soziale Realität.

Die \emph{vierte Form} ist eine sich erst ergebende Möglichkeit, die
Möglichkeit einer Simulation der sozialen Entwicklung. Sehr wohl ist es im
Prinzip möglich, eine derartige Verhaltensanalyse und Abgleichung zum Internet
der Dinge zu gewinnen, um Gründe für die Veränderung sozialer Kontexte zu
ermitteln. Diese Warum-Frage ist aber auf einer Ebene, bei der die
Verschränkung von Gesellschaftstheorie und Gesellschaftskritik hochgradig
heikel wird.

Die Gefahr eines Abrutschen in einen wirklich flachen Szientismus ist hier am
größten und erfordert geradezu eine \emph{fünfte Form} der Digital Humanities.
Diese ist in gewisser Weise das Programm, was wir hier skizzieren. Auf dieser
Ebene wird die Interdiziplinarität und Infradisziplinarität nicht nur für
diese Wissenschaftsdisziplin oder für die Wissenschaft allein entscheidend,
sondern für die politische Dimension jeder Analyse. Nicht Politikberatung ist
hier im Fokus, sondern die gesellschaftlich konstitutiven Narrative, welche
jenseits von Ordnungszusammenhängen unser Leben gestalten. Wir selbst kommen
als Agierende und sich Verhaltende in den Blick.

Mit einer Diskussion über die Sicherheit privater Daten oder über die
Einflussnahme großer Plattformen auf die Verfahrensweisen unseres
Zusammenlebens ist es hier nicht getan. Dieselbe philosophische Problematik
des Menschen-Weltbild-Komplexes hat zur zivilisatorischen Errungenschaft
bürgerlicher Freiheitsrechte geführt, und umfassende Digital Humanities können
sich nicht der technologischen Methodenbegrenzung dieser Einbettung
entziehen. Damit wird eine Reflexivität notwendig, welche zum einen erlaubt,
das eigene Paradigma anzuwenden, und zum anderen die gesellschaftskritischen
Implikationen nicht verkennt. Deswegen werden sich umfassende Digital
Humanites nicht auf technologische Spielereien oder auf digitalen
Behaviourismus beschränken können, sondern müssen Lehre und Forschung
interdisziplinär und infradisziplinär vereinigen, um ihren Forschern und
Studenten die Möglichkeit zu geben, sich ihres eigenen Verstandes zu
bedienen. Den Mut dazu braucht ein derartiges Vorhaben nicht zu geben, der
digitale Wandel erfordert diese mutige und vernünftig Gestaltung unserer
geteilten gesellschaftlichen Strukturen.

Eine so fundierte Propädeutik umfassender Digital Humanities kann allerdings
nur ein erster Ansatz sein \ldots

\ccnotiz
\end{document}
